{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook for ner_inf component\n",
    "\n",
    "### Critical Instructions\n",
    "\n",
    "1. Cell 1 contains component run parameters - change these as required when testing. These parameters will be passed automatically to the notebook when it is run on the xpresso server\n",
    "\n",
    "2. Cell 2 contains the component class code with the following methods. You may optionally overwrite these, as required;\n",
    "      - Constructor - you can overwrite the constructor if required (optional)\n",
    "      - on_pause - this is called by xpresso when the user pauses an experiment - use it to save your component state, and perform any cleanup required\n",
    "      - on_restart - this is called by xpresso when the user restarts an experiment - use it to reload your component state\n",
    "      - on_terminate - this is called by xpresso when the user terminates an experiment - use it to perform any cleanup required before stopping the notebook\n",
    "      - on_complete - this is called by xpresso when the proxy.completed method is called within the notebook. Use this to perform any completion operations (e.g., save the model to proxy.OUTPUT_DIR, any cleanup actions, etc.)\n",
    "\n",
    "### Execution Instructions\n",
    "1. Set the run parameters in Cell 1\n",
    "2. Implement any component methods you want in Cell 2 (optional)\n",
    "3. Run the first two cells\n",
    "4. Your component code should appear in subsequent cells;\n",
    "      - Use proxy.logger to log messages (see example provided)\n",
    "      - Use proxy.report_status to report status back to xpresso\n",
    "      - Use proxy.report_kpi_metrics to report KPIs back to xpresso\n",
    "      - Use proxy.report_timeseries_metrics to report timeseries data back to xpresso\n",
    "      - Use proxy.completed to indicate to xpresso that execution is complete (this will trigger a call to your component's on_complete method)\n",
    "\n",
    "#### Please reach out to xpresso.ai team for any issues or concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Variables in this cell are tagged as \"parameters\" &\n",
    "# will be replaced as command-line arguments during deployment\n",
    "\n",
    "# === Default command-line arguments ===\n",
    "enable_local_execution = True\n",
    "xpresso_run_name = \"Local Execution\"\n",
    "params_filename = None\n",
    "params_commit_id = None\n",
    "training_branch = None\n",
    "training_dataset = None\n",
    "training_data_version = None\n",
    "key_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generated python code for component. We define the proxy code that will be used for execution in this cell.\n",
    "    \n",
    "    Component flow is as follows\n",
    "    A. Local Execution - User is supposed to execute the NB on JupyterHub server for testing purposes.\n",
    "        1. The first 2 cells of the notebook should be run - these create the component object and a proxy object\n",
    "        2. Other cells can contain component code \n",
    "        3. Developer may make calls to proxy.report_status, proxy.report_kpi_metrics and \n",
    "            proxy.report_timeseries_metrics. These are mapped to local proxy methods which print information\n",
    "            on the console\n",
    "        4. Developer may make calls to proxy.completed method - this calls the on_complete method of the component\n",
    "            - Developer can implement this method to perform any cleanup tasks\n",
    "\n",
    "\n",
    "    B. Remote Execution - xpresso Controller calls xprbuild/system/linux/run.sh, which runs this notebook\n",
    "        1 The first 2 cells of the notebook should be run - these create the component object and a proxy object\n",
    "        2. Other cells can contain component code \n",
    "        3. Developer may make calls to proxy.report_status, proxy.report_kpi_metrics and \n",
    "            proxy.report_timeseries_metrics. These are mapped to remote proxy methods which display information\n",
    "            on the kubeflow console or xpresso UI as required\n",
    "        4. Developer may make calls to proxy.completed method - this calls the on_complete method of the component\n",
    "            - Developer can implement this method to perform any cleanup tasks\n",
    "        5. Developer may implement on_pause, on_restart and on_terminate to handle interrupts\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "__author__ = [\"### Author ###\"]\n",
    "\n",
    "\n",
    "class NerInf():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "            Note: Constructor cannot take any parameters\n",
    "            Initialize all the required constants and data here\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def on_pause(self, push_exp=False):\n",
    "        \"\"\"\n",
    "        This method is called by the local/remote proxy pause method\n",
    "        In remote execution, this would be called when a user pauses the pipeline from the UI\n",
    "        In local execution, this would be called from a separate thread for testing pause\n",
    "        Developers must implement any cleanup required in this method,\n",
    "        and save state of the program to disk\n",
    "        Args:\n",
    "            push_exp: Whether to push the data present in the output folder\n",
    "               to the versioning system. This is required once training is\n",
    "               completed and model needs to be versioned\n",
    "        \"\"\"\n",
    "        # === Your pause code goes here ===\n",
    "        pass\n",
    "\n",
    "    def on_restart(self):\n",
    "        \"\"\"\n",
    "        This method is called by the local/remote proxy restart method\n",
    "        In remote execution, this would be called when a user restarts the pipeline from the UI\n",
    "        In local execution, this would be called from a separate thread for testing restart\n",
    "        Developers should implement the logic to\n",
    "        reload the state of the previous run.\n",
    "        \"\"\"\n",
    "        # === Your restart code goes here ===\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def on_terminate(self):\n",
    "        \"\"\"\n",
    "        This method is called by the proxy terminate method\n",
    "        In remote execution, this would be called when a user terminates the pipeline from the UI\n",
    "        In local execution, this would be called from a separate thread for testing termination\n",
    "        Developers must implement any cleanup required in this method\n",
    "        \"\"\"\n",
    "        # === Your termination code goes here ===\n",
    "        pass\n",
    "    \n",
    "    def on_complete(self, push_exp=False, success=True):\n",
    "        \"\"\"\n",
    "        This method is called by the local/remote proxy completed method\n",
    "        Developers must implement any cleanup required in this method\n",
    "\n",
    "        Args:\n",
    "            push_exp: Whether to push the data present in the output folder\n",
    "               to the versioning system. This is required once training is\n",
    "               completed and model needs to be versioned\n",
    "            success: Use to handle failure cases\n",
    "        \"\"\"\n",
    "        # === Your completion code goes here ===\n",
    "        pass\n",
    "\n",
    "\n",
    "component_object = NerInf()\n",
    "\n",
    "# === Initialize Proxy ===\n",
    "if enable_local_execution:\n",
    "    from local_proxy import LocalProxy\n",
    "    proxy = LocalProxy(\n",
    "        component_object, xpresso_run_name, params_filename, params_commit_id)\n",
    "else:\n",
    "    from app.remote_proxy import RemoteProxy\n",
    "    proxy = RemoteProxy(\n",
    "        component_object, xpresso_run_name, params_filename, params_commit_id, \n",
    "        training_branch, training_dataset, training_data_version, key_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from polyfuzz import PolyFuzz\n",
    "from polyfuzz.models import Embeddings\n",
    "from flair.embeddings import TransformerWordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/project/pipelines/ner-topic/intermed_files/\"\n",
    "\n",
    "model_full_path = base_path + \"ner_topic_model.h5\"\n",
    "model = load_model(model_full_path)\n",
    "\n",
    "\n",
    "with open(base_path + 'tokenizer.pkl', 'rb') as fp_:\n",
    "    tokenizer = pickle.load(fp_)\n",
    "\n",
    "base_dic_df = pd.read_csv(base_path + 'entity_dic_filtered.csv')\n",
    "\n",
    "base_dic = {}\n",
    "for i in base_dic_df.index:\n",
    "    base_dic[base_dic_df['lemma_word_phrase_updated'][i]] = base_dic_df['derivedCat'][i]\n",
    "base_dic = base_dic\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "embeddings = TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
    "bert = Embeddings(embeddings, min_similarity=0, model_id=\"BERT\")\n",
    "models_ = PolyFuzz(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(chat):\n",
    "    chat['cleaned_message'] = chat['message'].apply(lambda x: \" \".join(x.lower().split()))\n",
    "    chat['tokenized'] = ''\n",
    "    for i in chat.index:\n",
    "        words_lis =  chat['cleaned_message'][i].split()\n",
    "        words_lis = [re.sub(\"[^A-Za-z0-9]+\", \" \", j) for j in words_lis]\n",
    "        words_lis = [re.sub(\"[0-9]+\", \" number\", j) for j in words_lis]\n",
    "        lem_words = [lemmatizer.lemmatize(j).lower() if len(j)>3 else j.lower() for j in words_lis]\n",
    "        chat['tokenized'][i] = \" \".join(lem_words).split()\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(chat,predictions,poly_dic):\n",
    "    chat['derived_topics'] = ''\n",
    "    chat['derived_topics_cat']=''\n",
    "    p = 0\n",
    "    fuz_list={}\n",
    "    poly_list = {}\n",
    "    for  i in chat.index:\n",
    "        out = predictions[p]\n",
    "        words_ = []\n",
    "        for j in range(0,len(out)):\n",
    "            if out[j]!='O':\n",
    "                if out[j]== 'B-entity':\n",
    "                    words_.append(chat['tokenized'][i][j])\n",
    "                elif out[j]== 'I-entity':\n",
    "                    if len(words_)>0:\n",
    "                        words_[len(words_)-1] = words_[len(words_)-1]+' '+ chat['tokenized'][i][j]\n",
    "                    else:\n",
    "                        words_.append(chat['tokenized'][i][j])\n",
    "        chat['derived_topics'][i] = words_\n",
    "        cat_ = []\n",
    "        for k in words_:\n",
    "            if k in base_dic.keys():\n",
    "                cat_.append(base_dic[k])\n",
    "            elif k in poly_dic.keys():\n",
    "                cat_.append(poly_dic[k])\n",
    "            else:\n",
    "                fuz_match = process.extractOne(k,base_dic.keys())\n",
    "                #if fuz_match[1]>85:\n",
    "                fuz_list[k]=fuz_match[0]\n",
    "                cat_.append(base_dic[fuz_match[0]])\n",
    "                #else:\n",
    "                #    poly_list[k]='tbd'\n",
    "                #    cat_.append('tbd')\n",
    "        chat['derived_topics_cat'][i] = cat_\n",
    "        p=p+1   \n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/project/pipelines/ner-inf-top/input_file/\"\n",
    "chat = pd.read_csv(input_path + \"input.csv\")\n",
    "chat = clean_message(chat)\n",
    "chat['tokenized_joined'] = chat['tokenized'].apply(lambda x: \" \".join(x))\n",
    "Max_Sequence_Length = 30\n",
    "seq = tokenizer.texts_to_sequences(chat['tokenized_joined'])\n",
    "data = pad_sequences(seq, maxlen = Max_Sequence_Length,padding='post',truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(data,batch_size = 128)\n",
    "ids_to_labels= {0: 'O',1: 'B-entity', 2: 'I-entity'}\n",
    "labels_to_ids = {'B-entity': 1, 'O': 0, 'I-entity': 2}\n",
    "predictions = []\n",
    "for i in range(0,len(pred)):\n",
    "    inner = ['O']*len(pred[i])\n",
    "    for j in range(0,len(inner)):\n",
    "        if data[i][j]==0:\n",
    "            break\n",
    "        inner[j] = ids_to_labels[np.argmax(pred[i][j])]\n",
    "    predictions.append(inner)\n",
    "keys = []\n",
    "for i in range(0,len(predictions)):\n",
    "    for j in range(0,len(predictions[i])):\n",
    "        if predictions[i][j] != 'O':\n",
    "            if predictions[i][j] == 'B-entity':\n",
    "                keys.append(chat['tokenized'][i][j])\n",
    "            elif predictions[i][j] == 'I-entity':\n",
    "                keys[len(keys)-1] = keys[len(keys)-1] + \" \"+ chat['tokenized'][i][j]\n",
    "\n",
    "fp = list(set(keys) - set(base_dic.keys()))\n",
    "poly_dic = {}   \n",
    "poly_df = pd.DataFrame()\n",
    "\n",
    "if len(fp)>0:\n",
    "    from_list = list(set(fp))\n",
    "    to_list = list(base_dic.keys())\n",
    "    models_.match(from_list, to_list)\n",
    "    poly_df = models_.get_matches()\n",
    "    poly_df['to_cate'] = poly_df['To'].apply(lambda x: base_dic[x])\n",
    "    for i in poly_df.index:\n",
    "        poly_dic[poly_df['From'][i]]= poly_df['to_cate'][i]\n",
    "\n",
    "chat = get_topics(chat,predictions,poly_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/project/pipelines/ner-inf-top/output_file/\"\n",
    "chat.to_csv(output_path + 'output_topic.csv',index=False)\n",
    "file_exists = exists(output_path + 'new_topic.csv')\n",
    "if file_exists==True:\n",
    "    poly_df_old = pd.read_csv(output_path + 'new_topic.csv')\n",
    "    if len(poly_df)>0:\n",
    "        poly_df_new = poly_df_old.append(poly_df).drop_duplicates(subset =['From'])\n",
    "        poly_df_new.to_csv(output_path + 'new_topic.csv',index=False)\n",
    "else:\n",
    "    if len(poly_df)>0:\n",
    "        poly_df.to_csv(output_path + 'new_topic.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to access run parameters\n",
    "#print(proxy.run_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# sample code for logger\n",
    "#proxy.logger.info(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to report KPI metrics\n",
    "#proxy.report_kpi_metrics({\"key\": 123})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to report Time-series metrics\n",
    "#proxy.report_timeseries_metrics({\"key\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to call completed method\n",
    "proxy.completed(push_exp=True, success=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (incident_similarity)",
   "language": "python",
   "name": "incident_similarity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "xpresso_branch_name": "xpresso-master",
  "xpresso_component_name": "ner_inf",
  "xpresso_project_name": "incident_similarity"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
